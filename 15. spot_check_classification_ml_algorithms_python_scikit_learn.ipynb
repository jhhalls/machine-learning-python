{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn\n",
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning problem.\n",
    "\n",
    "You cannot know which algorithms are best suited to your problem before hand. You must trial a number of methods and focus attention on those that prove themselves the most promising.\n",
    "\n",
    "In this post you will discover 6 machine learning algorithms that you can use when spot checking your classification problem in Python with scikit-learn.\n",
    "\n",
    "Letâ€™s get started.\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "We are going to take a look at 6 classification algorithms that you can spot check on your dataset.\n",
    "\n",
    "2 Linear Machine Learning Algorithms:\n",
    "* Logistic Regression\n",
    "* Linear Discriminant Analysis\n",
    "\n",
    "4 Nonlinear Machine Learning Algorithms:\n",
    "* K-Nearest Neighbors\n",
    "* Naive Bayes\n",
    "* Classification and Regression Trees\n",
    "* Support Vector Machines\n",
    "\n",
    "Each recipe is demonstrated on the Pima Indians onset of Diabetes dataset. This is a binary classification problem where all attributes are numeric.\n",
    "\n",
    "Each recipe is complete and standalone. This means that you can copy and paste it into your own project and start using it immediately.\n",
    "\n",
    "A test harness using 10-fold cross validation is used to demonstrate how to spot check each machine learning algorithm and mean accuracy measures are used to indicate algorithm performance.\n",
    "\n",
    "The recipes assume that you know about each machine learning algorithm and how to use them. We will not go into the API or parameterization of each algorithm.\n",
    "\n",
    "## Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms: logistic regression and linear discriminant analysis.\n",
    "\n",
    "### 1. Logistic Regression\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems.\n",
    "\n",
    "You can construct a logistic regression model using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76951469583\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "### 2. Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multi-class classification. It too assumes a Gaussian distribution for the numerical input variables.\n",
    "\n",
    "You can construct an LDA model using the [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "## Nonlinear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use 4 nonlinear machine learning algorithms.\n",
    "\n",
    "### 1. K-Nearest Neighbors\n",
    "K-Nearest Neighbors (or KNN) uses a distance metric to find the K most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction.\n",
    "\n",
    "You can construct a KNN model using the [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726555023923\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "random_state = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = KNeighborsClassifier()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "### 2. Naive Bayes\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption).\n",
    "\n",
    "When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the [Gaussian Probability Density Function](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "\n",
    "You can construct a Naive Bayes model using the [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75517771702\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GaussianNB()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "### 3. Classification and Regression Trees\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function.\n",
    "\n",
    "You can construct a CART model using the [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689969241285\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = DecisionTreeClassifier()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "### 4. Support Vector Machines\n",
    "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes.\n",
    "\n",
    "Of particular importance is the use of different kernel functions via the kernel parameter. A powerful [Radial Basis Function](https://en.wikipedia.org/wiki/Radial_basis_function) is used by default.\n",
    "\n",
    "You can construct an SVM model using the [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651025290499\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = SVC()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the mean estimated accuracy.\n",
    "\n",
    "## Summary\n",
    "In this post you discovered 6 machine learning algorithms that you can use to spot-check on your classification problem in Python using scikit-learn.\n",
    "\n",
    "Specifically, you learned how to spot-check:\n",
    "\n",
    "2 Linear Machine Learning Algorithms\n",
    "* Logistic Regression\n",
    "* Linear Discriminant Analysis\n",
    "\n",
    "4 Nonlinear Machine Learning Algorithms\n",
    "* K-Nearest Neighbors\n",
    "* Naive Bayes\n",
    "* Classification and Regression Trees\n",
    "* Support Vector Machines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
